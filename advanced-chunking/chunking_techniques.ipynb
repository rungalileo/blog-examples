{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union, Sequence\n",
    "from langchain_core.documents import BaseDocumentTransformer, Document\n",
    "\n",
    "class TextSplitter(BaseDocumentTransformer, ABC):\n",
    "    \"\"\"Interface for splitting text into chunks.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size: int = 4000,\n",
    "        chunk_overlap: int = 200,\n",
    "        length_function: Callable[[str], int] = len,\n",
    "        keep_separator: bool = False,\n",
    "        add_start_index: bool = False,\n",
    "        strip_whitespace: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new TextSplitter.\n",
    "\n",
    "        Args:\n",
    "            chunk_size: Maximum size of chunks to return\n",
    "            chunk_overlap: Overlap in characters between chunks\n",
    "            length_function: Function that measures the length of given chunks\n",
    "            keep_separator: Whether to keep the separator in the chunks\n",
    "            add_start_index: If `True`, includes chunk's start index in metadata\n",
    "            strip_whitespace: If `True`, strips whitespace from the start and end of\n",
    "                              every document\n",
    "        \"\"\"\n",
    "        if chunk_overlap > chunk_size:\n",
    "            raise ValueError(\n",
    "                f\"Got a larger chunk overlap ({chunk_overlap}) than chunk size \"\n",
    "                f\"({chunk_size}), should be smaller.\"\n",
    "            )\n",
    "        self._chunk_size = chunk_size\n",
    "        self._chunk_overlap = chunk_overlap\n",
    "        self._length_function = length_function\n",
    "        self._keep_separator = keep_separator\n",
    "        self._add_start_index = add_start_index\n",
    "        self._strip_whitespace = strip_whitespace\n",
    "\n",
    "    @abstractmethod\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into multiple components.\"\"\"\n",
    "\n",
    "    def _join_docs(self, docs: List[str], separator: str) -> Optional[str]:\n",
    "        text = separator.join(docs)\n",
    "        if self._strip_whitespace:\n",
    "            text = text.strip()\n",
    "        if text == \"\":\n",
    "            return None\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def _merge_splits(self, splits: Iterable[str], separator: str) -> List[str]:\n",
    "        # We now want to combine these smaller pieces into medium size\n",
    "        # chunks to send to the LLM.\n",
    "        separator_len = self._length_function(separator)\n",
    "\n",
    "        docs = []\n",
    "        current_doc: List[str] = []\n",
    "        total = 0\n",
    "        for d in splits:\n",
    "            _len = self._length_function(d)\n",
    "            if (\n",
    "                total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                > self._chunk_size\n",
    "            ):\n",
    "                if total > self._chunk_size:\n",
    "                    logger.warning(\n",
    "                        f\"Created a chunk of size {total}, \"\n",
    "                        f\"which is longer than the specified {self._chunk_size}\"\n",
    "                    )\n",
    "                if len(current_doc) > 0:\n",
    "                    doc = self._join_docs(current_doc, separator)\n",
    "                    if doc is not None:\n",
    "                        docs.append(doc)\n",
    "                    # Keep on popping if:\n",
    "                    # - we have a larger chunk than in the chunk overlap\n",
    "                    # - or if we still have any chunks and the length is long\n",
    "                    while total > self._chunk_overlap or (\n",
    "                        total + _len + (separator_len if len(current_doc) > 0 else 0)\n",
    "                        > self._chunk_size\n",
    "                        and total > 0\n",
    "                    ):\n",
    "                        total -= self._length_function(current_doc[0]) + (\n",
    "                            separator_len if len(current_doc) > 1 else 0\n",
    "                        )\n",
    "                        current_doc = current_doc[1:]\n",
    "            current_doc.append(d)\n",
    "            total += _len + (separator_len if len(current_doc) > 1 else 0)\n",
    "        doc = self._join_docs(current_doc, separator)\n",
    "        if doc is not None:\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "    def transform_documents(\n",
    "        self, documents: Sequence[Document], **kwargs: Any\n",
    "    ) -> Sequence[Document]:\n",
    "        \"\"\"Transform sequence of documents by splitting them.\"\"\"\n",
    "        return self.split_documents(list(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is first. This is second. This is third. This is fourth. This is fifth.\\n\\nThis is sixth. This is seventh. This is eighth. This is ninth. This is tenth.\"\n",
    "\n",
    "python_code_text = \"\"\"\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.age = 0\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def get_age(self):\n",
    "        return self.age\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is first. This is second. This is t',\n",
       " 'is second. This is third. This is fourth',\n",
       " 'hird. This is fourth. This is fifth.\\n\\nTh',\n",
       " '. This is fifth.\\n\\nThis is sixth. This is',\n",
       " 'is is sixth. This is seventh. This is ei',\n",
       " 'seventh. This is eighth. This is ninth.',\n",
       " 'ghth. This is ninth. This is tenth.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import _split_text_with_regex\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "class CharacterTextSplitter(TextSplitter):\n",
    "    \"\"\"Splitting text that looks at characters.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, separator: str = \"\\n\\n\", is_separator_regex: bool = False, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new TextSplitter.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self._separator = separator\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        # First we naively split the large input into a bunch of smaller ones.\n",
    "        separator = (\n",
    "            self._separator if self._is_separator_regex else re.escape(self._separator)\n",
    "        )\n",
    "        splits = _split_text_with_regex(text, separator, self._keep_separator)\n",
    "        _separator = \"\" if self._keep_separator else self._separator\n",
    "        return self._merge_splits(splits, _separator)\n",
    "\n",
    "splitter = CharacterTextSplitter(separator=\"\", chunk_size=40, chunk_overlap=20)\n",
    "splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is first. This is second. This is t',\n",
       " 'is second. This is third. This is fourth',\n",
       " 'hird. This is fourth. This is fifth.',\n",
       " 'This is sixth. This is seventh. This i',\n",
       " 's is seventh. This is eighth. This is ni',\n",
       " 's eighth. This is ninth. This is tenth.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RecursiveCharacterTextSplitter(TextSplitter):\n",
    "    \"\"\"Splitting text by recursively look at characters.\n",
    "\n",
    "    Recursively tries to split by different characters to find one\n",
    "    that works.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        separators: Optional[List[str]] = None,\n",
    "        keep_separator: bool = True,\n",
    "        is_separator_regex: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new TextSplitter.\"\"\"\n",
    "        super().__init__(keep_separator=keep_separator, **kwargs)\n",
    "        self._separators = separators or [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "\n",
    "    def _split_text(self, text: str, separators: List[str]) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        final_chunks = []\n",
    "        # Get appropriate separator to use\n",
    "        separator = separators[-1]\n",
    "        new_separators = []\n",
    "        for i, _s in enumerate(separators):\n",
    "            _separator = _s if self._is_separator_regex else re.escape(_s)\n",
    "            if _s == \"\":\n",
    "                separator = _s\n",
    "                break\n",
    "            if re.search(_separator, text):\n",
    "                separator = _s\n",
    "                new_separators = separators[i + 1 :]\n",
    "                break\n",
    "\n",
    "        _separator = separator if self._is_separator_regex else re.escape(separator)\n",
    "        splits = _split_text_with_regex(text, _separator, self._keep_separator)\n",
    "\n",
    "        # Now go merging things, recursively splitting longer texts.\n",
    "        _good_splits = []\n",
    "        _separator = \"\" if self._keep_separator else separator\n",
    "        for s in splits:\n",
    "            if self._length_function(s) < self._chunk_size:\n",
    "                _good_splits.append(s)\n",
    "            else:\n",
    "                if _good_splits:\n",
    "                    merged_text = self._merge_splits(_good_splits, _separator)\n",
    "                    final_chunks.extend(merged_text)\n",
    "                    _good_splits = []\n",
    "                if not new_separators:\n",
    "                    final_chunks.append(s)\n",
    "                else:\n",
    "                    other_info = self._split_text(s, new_separators)\n",
    "                    final_chunks.extend(other_info)\n",
    "        if _good_splits:\n",
    "            merged_text = self._merge_splits(_good_splits, _separator)\n",
    "            final_chunks.extend(merged_text)\n",
    "        return final_chunks\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        return self._split_text(text, self._separators)\n",
    "\n",
    "    @classmethod\n",
    "    def from_language(\n",
    "        cls, language: Language, **kwargs: Any\n",
    "    ):\n",
    "        separators = cls.get_separators_for_language(language)\n",
    "        return cls(separators=separators, is_separator_regex=True, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_separators_for_language(language: Language) -> List[str]:\n",
    "        if language == Language.PYTHON:\n",
    "            return [\n",
    "                # First, try to split along class definitions\n",
    "                \"\\nclass \",\n",
    "                \"\\ndef \",\n",
    "                \"\\n\\tdef \",\n",
    "                # Now split by the normal type of lines\n",
    "                \"\\n\\n\",\n",
    "                \"\\n\",\n",
    "                \" \",\n",
    "                \"\",\n",
    "            ]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\\n\", \"\"], chunk_size=40, chunk_overlap=20)\n",
    "splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 40, 36, 38, 40, 39]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in splitter.split_text(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "import os\n",
      "import sys\n",
      "import json\n",
      "\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "class MyClass:\n",
      "    def __init__(self, name):\n",
      "        self.name = name\n",
      "        self.age = 0\n",
      "\n",
      "    def get_name(self):\n",
      "        return self.name\n",
      "\n",
      "    def get_age(self):\n",
      "        return self.age\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter.from_language(Language.PYTHON)\n",
    "\n",
    "for i,chunk in enumerate(splitter.split_text(python_code_text)):\n",
    "    print(i)\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "class SpacySentenceTokenizer:\n",
    "    def __init__(self, spacy_model=\"en_core_web_sm\"):\n",
    "        self.nlp = spacy.load(spacy_model)\n",
    "\n",
    "    def create_documents(\n",
    "        self, documents, metadatas=None, overlap: int = 0, stride: int = 1\n",
    "    )-> List[Document]:\n",
    "        chunks = []\n",
    "        if not metadatas:\n",
    "            metadatas = [{}] * len(documents)\n",
    "        for doc, metadata in zip(documents, metadatas):\n",
    "            text_chunks = self.split_text(doc, overlap, stride)\n",
    "            for chunk_text in text_chunks:\n",
    "                chunks.append(Document(page_content=chunk_text, metadata=metadata))\n",
    "        return chunks\n",
    "\n",
    "    def split_text(self, text: str, stride: int = 1, overlap: int = 0) -> List[str]:\n",
    "        sentences = list(self.nlp(text).sents)\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), stride):\n",
    "            chunk_text = \" \".join(str(sent) for sent in sentences[i : i + overlap + 1])\n",
    "            chunks.append(chunk_text)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love dogs. They are amazing. Cats must be the easiest pets around.',\n",
       " 'They are amazing. Cats must be the easiest pets around. Tesla robots are advanced now with AI.',\n",
       " 'Cats must be the easiest pets around. Tesla robots are advanced now with AI. They will take us to mars.',\n",
       " 'Tesla robots are advanced now with AI. They will take us to mars.',\n",
       " 'They will take us to mars.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love dogs. They are amazing. Cats must be the easiest pets around. Tesla robots are advanced now with AI. They will take us to mars.\"\n",
    "  \n",
    "tokenizer = SpacySentenceTokenizer()\n",
    "chunks = tokenizer.split_text(text, stride=1, overlap=2)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love dogs. They are amazing.',\n",
       " 'Cats must be the easiest pets around.',\n",
       " 'Tesla robots are advanced now with AI. They will take us to mars.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "\n",
    "\n",
    "class SentenceTransformersSimilarity:\n",
    "    def __init__(self, model=\"all-MiniLM-L6-v2\", similarity_threshold=0.2):\n",
    "        self.model = SentenceTransformer(model)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def similarities(self, sentences: List[str]):\n",
    "        # Encode all sentences\n",
    "        embeddings = self.model.encode(sentences)\n",
    "\n",
    "        # Calculate cosine similarities for neighboring sentences\n",
    "        similarities = []\n",
    "        for i in range(1, len(embeddings)):\n",
    "            sim = util.pytorch_cos_sim(embeddings[i - 1], embeddings[i]).item()\n",
    "            similarities.append(sim)\n",
    "\n",
    "        return similarities\n",
    "\n",
    "\n",
    "class SpacySentenceSplitter():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def split(self, text: str) -> List[str]:\n",
    "        doc = self.nlp(text)\n",
    "        return [str(sent).strip() for sent in doc.sents]\n",
    "\n",
    "\n",
    "class SimilarSentenceSplitter():\n",
    "\n",
    "    def __init__(self, similarity_model, sentence_splitter):\n",
    "        self.model = similarity_model\n",
    "        self.sentence_splitter = sentence_splitter\n",
    "\n",
    "    def split_text(self, text: str, group_max_sentences=5) -> List[str]:\n",
    "        \"\"\"\n",
    "        group_max_sentences: The maximum number of sentences in a group.\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_splitter.split(text)\n",
    "\n",
    "        if len(sentences) == 0:\n",
    "            return []\n",
    "\n",
    "        similarities = self.model.similarities(sentences)\n",
    "\n",
    "        # The first sentence is always in the first group.\n",
    "        groups = [[sentences[0]]]\n",
    "\n",
    "        # Using the group min/max sentences contraints,\n",
    "        # group together the rest of the sentences.\n",
    "        for i in range(1, len(sentences)):\n",
    "            if len(groups[-1]) >= group_max_sentences:\n",
    "                groups.append([sentences[i]])\n",
    "            elif similarities[i - 1] >= self.model.similarity_threshold:\n",
    "                groups[-1].append(sentences[i])\n",
    "            else:\n",
    "                groups.append([sentences[i]])\n",
    "\n",
    "        return [\" \".join(g) for g in groups]\n",
    "\n",
    "\n",
    "text = \"I love dogs. They are amazing. Cats must be the easiest pets around. Tesla robots are advanced now with AI. They will take us to mars.\"\n",
    "\n",
    "model = SentenceTransformersSimilarity()\n",
    "sentence_splitter = SpacySentenceSplitter()\n",
    "splitter = SimilarSentenceSplitter(model, sentence_splitter)\n",
    "splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.staging.base import elements_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/gemini_v1.5.pdf\"\n",
    "\n",
    "# Extracts the elements from the PDF\n",
    "elements = partition_pdf(\n",
    "    filename=filename,\n",
    "    # Unstructured Helpers\n",
    "    strategy=\"hi_res\",\n",
    "    infer_table_structure=True,\n",
    "    model_name=\"yolox\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(elements)):\n",
    "#     if \"Table\" in elements[i].__repr__():\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><th></th><th>Context length</th><th>AutoAIS Gemini 1.5 Pro</th><th>AIS Human Evaluation</th><th>Num. Sentences per answer</th></thead><tr><td>Anthropic Claude 2.1</td><td>0-shot</td><td>11.1</td><td>30.2</td><td>5.7</td></tr><tr><td>Gemini 1.0 Pro</td><td>0-shot</td><td>85.3</td><td>79.1</td><td>2.3</td></tr><tr><td>Gemini 1.5 Pro</td><td>0-shot</td><td>82.1</td><td>75.5</td><td>3.4</td></tr><tr><td>Anthropic Claude 2.1</td><td>4k retrieved</td><td>29.1</td><td>42.2</td><td>5.1</td></tr><tr><td>Gemini 1.0 Pro</td><td>4k retrieved</td><td>75.3</td><td>72.1</td><td>2.6</td></tr><tr><td>Gemini 1.5 Pro</td><td>4k retrieved</td><td>84.8</td><td>78.2</td><td>4.9</td></tr><tr><td>Gemini 1.5 Pro</td><td>710k book</td><td>91.4</td><td>80.0</td><td>5.8</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table = elements[149].metadata.text_as_html\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(table))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
