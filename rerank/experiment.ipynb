{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time, glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Pinecone as langchain_pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "import pandas as pd\n",
    "import promptquality as pq\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from metrics import all_metrics\n",
    "from qa_chain import get_qa_chain\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/nvidia_10k_2023.pdf\n",
      "../data/nvidia_10k_2022.pdf\n",
      "../data/nvidia_10k_2021.pdf\n",
      "../data/nvidia_10k_2024.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "701"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for file_path in glob.glob(\"../data/nvidia_10k_*.pdf\"):\n",
    "    print(file_path)\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    documents.extend(loader.load_and_split())\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(text):\n",
    "    questions = chat_model.invoke(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\"Your job is to generate only 1 short question from the given text such that it can be answered using the provided text. Use the exact info in the questions as mentioned in the text. There should not be duplicates questions. Return questions starting with - instead of numbers.\n",
    "\n",
    "Text: {text}\n",
    "Questions: \"\"\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "    questions = questions.content.replace(\"- \", \"\").split(\"\\n\")\n",
    "    questions = list(filter(None, questions)) \n",
    "    return questions\n",
    "\n",
    "text = documents[1].page_content\n",
    "print(text)\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=1.0)\n",
    "get_questions(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"text\": [doc.page_content for doc in documents]})\n",
    "df = df.sample(n=100, random_state=0)\n",
    "df[\"questions\"] = df.text.progress_apply(get_questions)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/nvidia_questions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Automotive market platform, COVID-19 did not h...</td>\n",
       "      <td>[In which regions did COVID-19 lead to an incr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that liabilities, while possible, are not prob...</td>\n",
       "      <td>[Can the possible loss or range of loss in leg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Table of Contents\\nItem 7. Management's Discus...</td>\n",
       "      <td>[What are NVIDIA's two operating segments as m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to  Note  12  of  the  Notes  to  the  Consoli...</td>\n",
       "      <td>[What is the amount of long-term tax liabiliti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(5)    La Compa√±√≠a y sus Afiliadas no son resp...</td>\n",
       "      <td>[Where are the offices of the Company register...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Automotive market platform, COVID-19 did not h...   \n",
       "1  that liabilities, while possible, are not prob...   \n",
       "2  Table of Contents\\nItem 7. Management's Discus...   \n",
       "3  to  Note  12  of  the  Notes  to  the  Consoli...   \n",
       "4  (5)    La Compa√±√≠a y sus Afiliadas no son resp...   \n",
       "\n",
       "                                           questions  \n",
       "0  [In which regions did COVID-19 lead to an incr...  \n",
       "1  [Can the possible loss or range of loss in leg...  \n",
       "2  [What are NVIDIA's two operating segments as m...  \n",
       "3  [What is the amount of long-term tax liabiliti...  \n",
       "4  [Where are the offices of the Company register...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/nvidia_questions.csv\")\n",
    "df[\"questions\"] = df.questions.apply(eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = df.explode(\"questions\").questions.sample(n=100, random_state=1).tolist()\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a Pinecone client\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "project_name = \"emb-model-eval\"\n",
    "temperature = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëã You have logged into üî≠ Galileo (https://console.demo.rungalileo.io/) as pratik@rungalileo.io.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Config(console_url=Url('https://console.demo.rungalileo.io/'), username='pratik@rungalileo.io', password=SecretStr('**********'), api_key=None, token=SecretStr('**********'), current_user='pratik@rungalileo.io', current_project_id=None, current_project_name=None, current_run_id=None, current_run_name=None, current_run_url=None, current_run_task_type=None, current_template_id=None, current_template_name=None, current_template_version_id=None, current_template_version=None, current_template=None, current_dataset_id=None, current_job_id=None, api_url=Url('https://api.demo.rungalileo.io/'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_chain_executor(emb_model_name: str, dimensions: int, llm_model_name: str, emb_k: int, rerank_k: int) -> None:\n",
    "    # initialise embedding model\n",
    "    if \"text-embedding-3\" in emb_model_name:\n",
    "        embeddings = OpenAIEmbeddings(model=emb_model_name, dimensions=dimensions)\n",
    "    else:\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=emb_model_name, encode_kwargs = {'normalize_embeddings': True})\n",
    "        \n",
    "    index_name = f\"{emb_model_name}-{dimensions}\".lower()\n",
    "    \n",
    "    # First, check if our index already exists and delete stale index\n",
    "    if index_name in [index_info['name'] for index_info in pc.list_indexes()]:\n",
    "        pc.delete_index(index_name)\n",
    "\n",
    "    # create a new index\n",
    "    pc.create_index(name=index_name, metric=\"cosine\", dimension=dimensions, \n",
    "                    spec=ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-west-2\"\n",
    "                ) )\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # index the documents\n",
    "    _ = langchain_pinecone.from_documents(documents, embeddings, index_name=index_name)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # load qa chain \n",
    "    qa = get_qa_chain(embeddings, index_name, emb_k, rerank_k, llm_model_name, temperature)\n",
    "    \n",
    "    # tags to be kept in galileo run\n",
    "    run_name = f\"{index_name}-emb-k-{emb_k}-rerank-k-{rerank_k}\"\n",
    "    index_name_tag = pq.RunTag(key=\"Index config\", value=index_name, tag_type=pq.TagType.RAG)\n",
    "    encoder_model_name_tag = pq.RunTag(key=\"Encoder\", value=emb_model_name, tag_type=pq.TagType.RAG)\n",
    "    llm_model_name_tag = pq.RunTag(key=\"LLM\", value=llm_model_name, tag_type=pq.TagType.RAG)\n",
    "    dimension_tag = pq.RunTag(key=\"Dimension\", value=str(dimensions), tag_type=pq.TagType.RAG)\n",
    "    emb_k_tag = pq.RunTag(key=\"Emb k\", value=str(emb_k), tag_type=pq.TagType.RAG)\n",
    "    rerank_k_tag = pq.RunTag(key=\"Rerank k\", value=str(rerank_k), tag_type=pq.TagType.RAG)\n",
    "\n",
    "    evaluate_handler = pq.GalileoPromptCallback(project_name=project_name, run_name=run_name, scorers=all_metrics, run_tags=[encoder_model_name_tag, llm_model_name_tag, index_name_tag, dimension_tag, emb_k_tag, rerank_k_tag])\n",
    "\n",
    "    # run chain with questions to generate the answers\n",
    "    print(\"Ready to ask!\")\n",
    "    for i, q in enumerate(tqdm(questions)):\n",
    "        print(f\"Question {i}: \", q)\n",
    "        print(qa.invoke(q, config=dict(callbacks=[evaluate_handler])))\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    evaluate_handler.finish()\n",
    "    \n",
    "pq.login(\"console.demo.rungalileo.io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.sweep(\n",
    "    rag_chain_executor,\n",
    "    {\n",
    "        \"emb_model_name\": [\"text-embedding-3-small\"],\n",
    "        \"dimensions\": [384],\n",
    "        \"llm_model_name\": [\"gpt-3.5-turbo-0125\"],\n",
    "        \"emb_k\": [10],\n",
    "        \"rerank_k\": [3]\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
